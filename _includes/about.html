<section id="about" class="container-outer dark fit-page">
  <div class="container">
    <span class="main-text">
      Edge Benchmark measures speed of your Machine Learning models on a wide variety of real devices.
    </span>

    <hr class="divider light">

    <span class="long-text">
      We all know it. We are in the middle of the expansion of Machine Learning models for edge devices and we need to know how fast would the models be when deployed on <strong>real devices</strong>. But in order to measure the speed, we have to convert our models to special format, connect the testing devices with the right prebuilt inference engine on it, upload them through <i>adb</i>, run the benchmarks and then try to figure out how fast the models are or where the potential bottleneck might be hidden, all only from the cryptic output of benchmark in terminal.
      <br>
      <br>
      <br>
      <strong style="font-size: 1.5em;">Wouldn't it be nice if this process of model benchmarking would be easier?</strong>
      <br>
    </span>

  </div>
</section>
